<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Analyse de Sentiments de Tweets : Approches de Modélisation et Implémentation MLOps</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }
        .container {
            width: 80%;
            margin: auto;
            overflow: hidden;
            padding: 20px;
            background-color: #fff;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        header {
            background: #2c3e50; /* Couleur plus neutre */
            color: #ffffff;
            padding-top: 30px;
            min-height: 70px;
            border-bottom: #3498db 3px solid; /* Couleur plus neutre */
            text-align: center;
        }
        header h1 {
            margin: 0;
            font-size: 2.2em;
        }
        article h2 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 5px;
            margin-top: 30px;
        }
        article h3 {
            color: #2c3e50;
            margin-top: 25px;
        }
        .screenshot {
            display: block;
            max-width: 90%; /* Un peu moins large pour mieux s'intégrer */
            height: auto;
            margin: 20px auto;
            border: 1px solid #ddd;
            box-shadow: 1px 1px 3px rgba(0,0,0,0.1);
        }
        figcaption {
            text-align: center;
            font-style: italic;
            color: #555; /* Un peu plus foncé */
            margin-bottom: 15px;
            font-size: 0.9em;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 10px; /* Un peu plus d'espace */
            text-align: left;
        }
        th {
            background-color: #ecf0f1; /* Couleur plus neutre */
            font-weight: bold;
        }
        .code-snippet {
            background-color: #f9f9f9; /* Fond plus clair */
            border: 1px solid #e0e0e0; /* Bordure plus discrète */
            border-left: 3px solid #3498db;
            padding: 15px;
            margin: 15px 0;
            font-family: "Courier New", Courier, monospace;
            font-size: 0.95em;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        footer {
            text-align: center;
            padding: 15px;
            margin-top: 30px;
            color: #7f8c8d; /* Gris plus clair */
            background: #ecf0f1;
            font-size: 0.9em;
        }
        .introduction, .conclusion {
            background-color: #f9f9f9;
            padding: 15px;
            border-left: 3px solid #3498db;
            margin-bottom: 20px;
        }
    </style>
</head>
<body>
    <header>
        <h1>Analyse de Sentiments de Tweets : Approches de Modélisation et Implémentation MLOps</h1>
    </header>

    <div class="container">
        <article>
            <p><em>Auteur : LECROART Antoine</em></p>

            <div class="introduction">
                <p>
                    Ce document détaille le développement d'un système d'analyse de sentiments pour des tweets, en explorant différentes approches de modélisation, 
                    de la régression logistique aux réseaux de neurones profonds, y compris l'utilisation de BERT. 
                    Une attention particulière est portée à l'implémentation d'une démarche MLOps pour assurer la reproductibilité, le suivi des expérimentations, 
                    le déploiement continu et le monitoring en production du modèle sélectionné. L'objectif était de créer un prototype fonctionnel et 
                    de mettre en place les fondations pour une solution d'IA robuste et évolutive.
                </p>
            </div>

            <h2>1. Contexte et Données</h2>
            <p>
                Le projet vise à prédire le sentiment (négatif ou positif) exprimé dans des tweets. 
                Un jeu de données public <a href="https://www.kaggle.com/datasets/kazanova/sentiment140">(source)</a> a été utilisé. 
                Ce dataset inclut l'heure, l'auteur (id et username), le contenu du tweet et le sentiment associé.
                Une première phase de prétraitement à réduit ce dataset aux deux colonnes qui nous intéresses : le tweet et le sentiment.
            </p>

            <h2>2. Exploration des Modèles d'Analyse de Sentiments</h2>
            <p>Trois principales approches de modélisation ont été évaluées :</p>

            <h3>2.1. Modèle de Référence : Régression Logistique</h3>
            <p>
                Un modèle de régression logistique a été implémenté comme baseline. Les tweets ont été vectorisés en utilisant une représentation TF-IDF (Term Frequency-Inverse Document Frequency).
                Un test préliminaire à permis d'évaluer que les résultats étaient meilleurs sans prétraitement des données.
            </p>
            <p>
                <strong>Paramètres clés et résultats :</strong>
                <ul>
                    <li>Vectorizer : TF-IDF (<code>ngram_range=(1, 3)</code>, <code>max_features=10000</code>)</li>
                    <li>Modèle : LogisticRegression (<code>solver='saga'</code>, <code>C=0.5</code, >, <code>max iter=500</code>, <code>class weight=balanced</code>)</li>
                    <li>Performance (Test) : F1-score = <code>0.80</code>, Accuracy = <code>0.80</code>.</li>
                </ul>
            </p>
            <figure>
                <img src="img/mlflow_simple.png" alt="Métriques MLflow pour la Régression Logistique" class="screenshot">
                <figcaption>Figure 1 : Suivi des métriques de la Régression Logistique via MLflow.</figcaption>
            </figure>

            <h3>2.2. Modèle Avancé : Réseau de Neurones Récurrents (LSTM)</h3>
            <p>
                Pour une meilleure capture des dépendances séquentielles et du contexte, un modèle basé sur des couches LSTM (Long Short-Term Memory) a été développé.
                Un test préliminaire à permis d'évaluer que les résultats étaient meilleurs avec prétraitement des données 
                (suppression des URLs, mentions, caractères spéciaux, lemmatization, decontraction,  ect...) mais sans suppression des stop words.
            </p>
            <p>
                <strong>Embeddings de Mots :</strong> Deux types d'embeddings ont été testés : GloVe et embeddings vierges. 
                Les embeddings GloVe (<code>GloVe.6B.300d</code>) ont fourni les meilleurs résultats préliminaires et ont été retenus. 
                Le vocabulaire a été limité aux <code>300</code> mots les plus fréquents et les séquences paddées/tronquées à une longueur de <code>20</code>.
            </p>
            <p>
                
                <figure>
                    <img src="img/advanced_architecture.png" alt="Architecture du modèle LSTM" class="screenshot">
                <figcaption>Figure 2 : Architecture simplifié du modèle LSTM.</figcaption>
            </figure>
            </p>
            <p>
                <strong>Performance (Test) :</strong> F1-score = <code>0.76</code>, Accuracy = <code>0.76</code>.
            </p>
            <figure>
                <img src="img/mlflow_advanced.png" alt="Métriques MLflow pour le modèle LSTM" class="screenshot">
                <figcaption>Figure 2 : Suivi des métriques du modèle LSTM via MLflow.</figcaption>
            </figure>

            <h3>2.3. Modèle Basé sur Transformers : BERT</h3>
            <p>
                Une approche utilisant BERT (Bidirectional Encoder Representations from Transformers), spécifiquement <code>bert-base-uncased</code>, a été explorée. Le modèle pré-entraîné a été fine-tuné sur le dataset de tweets.
            </p>
            <p>
                <strong>Performance (Test) :</strong> F1-score = <code>0.74</code>, Accuracy = <code>0.74</code>.
            </p>
            <figure>
                <img src="img/mlflow_bert.png" alt="Métriques MLflow pour le modèle BERT" class="screenshot">
                <figcaption>Figure 3 : Suivi des métriques du modèle BERT fine-tuné via MLflow.</figcaption>
            </figure>

            <h3>2.4. Synthèse des Performances</h3>
            <table>
                <thead>
                    <tr>
                        <th>Modèle</th>
                        <th>F1-score (Ensemble de Test)</th>
                        <th>Accuracy (Ensemble de Test)</th>
                        <th>Principaux Paramètres / Complexité</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Régression Logistique</td>
                        <td><code>0.80</code></td>
                        <td><code>0.80</code></td>
                        <td>TF-IDF, faible complexité</td>
                    </tr>
                    <tr>
                        <td>LSTM + <code>GloVe</code></td>
                        <td><code>0.76</code></td>
                        <td><code>0.76</code></td>
                        <td>Embeddings pré-entraînés GloVe, complexité moyenne</td>
                    </tr>
                    <tr>
                        <td>BERT (<code>bert-base-uncased</code>)</td>
                        <td><code>0.74</code></td>
                        <td><code>0.74</code></td>
                        <td>Transformers, haute complexité</td>
                    </tr>
                </tbody>
            </table>

            <h2>3. Implémentation de la Démarche MLOps</h2>
            <p>
                Une démarche MLOps a été mise en œuvre pour structurer le cycle de vie du modèle, de l'expérimentation au monitoring.
            </p>

            <h3>3.1. Suivi des Expérimentations avec MLflow</h3>
            <p>
                MLflow a été utilisé pour tracker les paramètres, les métriques (accuracy, F1-score, loss), les artefacts (modèles sauvegardés, graphiques) et le code source de chaque exécution d'entraînement. Cela permet une comparaison rigoureuse des modèles et la reproductibilité des résultats.
            </p>
            <figure>
                <img src="img/tracking_mlflow.png" alt="Interface MLflow pour la comparaison des runs" class="screenshot">
                <figcaption>Figure 4 : Interface MLflow montrant la comparaison de plusieurs exécutions d'entraînement.</figcaption>
            </figure>
            <p>Les modèles finaux ont été enregistrés dans le registre de modèles MLflow pour une meilleure gestion des versions.</p>

            <h3>3.2. Versioning du Code avec Git et GitHub</h3>
            <p>
                L'ensemble du code source (notebooks Jupyter pour l'exploration et l'entraînement, scripts Python pour l'API et les tests) est versionné avec Git et hébergé sur un dépôt GitHub. La structure du projet est organisée pour séparer la logique de modélisation, le code de l'application API et les tests.
            </p>
            <figure>
                <img src="img/structure_projet.png" alt="Structure du projet sur GitHub" class="screenshot">
                <figcaption>Figure 5 : Organisation des dossiers du projet sur GitHub.</figcaption>
            </figure>

            <h3>3.3. Tests Unitaires pour l'API</h3>
            <p>
                Des tests unitaires ont été écrits avec `pytest` et `TestClient` de FastAPI pour valider le comportement de l'API de prédiction. Ces tests couvrent les réponses attendues pour des entrées valides et la gestion des cas limites.
            </p>
            <div class="code-snippet"><pre><code>
# Extrait de api/test_main.py
# Teste la prédiction de sentiment positif.
def test_predict_positive_sentiment():
    response = client.post("/predict", json={"text": "I love this company ! the service was excellent !!!"})
    assert response.status_code == 200
    data = response.json()
    assert "sentiment" in data
    assert data["sentiment"] == "positif"
            </code></pre></div>

            <h3>3.4. Pipeline de Déploiement Continu (CI/CD)</h3>
            <p>
                Un pipeline CI/CD a été configuré avec GitHub Actions. À chaque `push` sur la branche `main` :
                <ol>
                        <li>Push sur la branche main : C'est l'événement qui déclenche l'ensemble du processus.</li>
                        <li>Job: Deploy : Le conteneur principal qui exécute toutes les étapes sur une machine virtuelle (runner) Ubuntu.</li>
                        <li>Préparation de l'environnement : Cette boîte regroupe les premières étapes qui se déroulent sur le runner de GitHub Actions : récupération du code, configuration de Python et installation des dépendances nécessaires pour les tests.</li>
                        <li>Exécution des tests : Une étape cruciale de validation. Si les tests (pytest) échouent, le pipeline s'arrête ici.</li>
                        <li>Synchronisation avec le serveur : Si les tests réussissent, le pipeline configure la connexion SSH et copie les fichiers de l'application (le contenu du dossier api/) vers votre serveur de production via scp.</li>
                        <li>
                            Déploiement sur le serveur : C'est le cœur du déploiement. Une fois les fichiers sur le serveur, une série de commandes sont exécutées à distance pour :
                            <ol>
                                <li>Créer le fichier d'environnement .env à partir des secrets GitHub.</li>
                                <li>Créer ou recréer un environnement virtuel Python (venv) propre.</li>           
                                <li>Installer les dépendances de l'application dans ce venv.</li>       
                                <li>Redémarrer le service (systemd) pour que la nouvelle version de l'application soit en ligne.</li>                        
                            </ol>
                        </li>
                        <li>Déploiement terminé : Le point final, indiquant que le processus a réussi.</li> 
                </ol>
            </p>
            <figure>
                <img src="img/cicd.png" alt="Workflow GitHub Actions pour le CI/CD" class="screenshot">
                <figcaption>Figure 6 : Visualisation du workflow CI/CD dans GitHub Actions.</figcaption>
            </figure>

            <h3>3.5. Exposition du Modèle via une API</h3>
            <p>
                Le modèle LSTM sélectionné est exposé via une API RESTful développée avec FastAPI. L'API prend en entrée un texte de tweet et retourne la prédiction de sentiment ainsi que la probabilité associée. Le chargement du modèle et du tokenizer est géré au démarrage de l'application.
            </p>

            <h3>3.6. Monitoring en Production et Collecte de Feedback</h3>
            <p>
                Le suivi du modèle en production est initié via Azure Application Insights.
                <ul>
                    <li><strong>Collecte de feedback utilisateur :</strong> Une interface locale (Streamlit) permet aux utilisateurs de tester l'API et de signaler les prédictions incorrectes. Ces signalements (texte du tweet, prédiction du modèle) sont envoyés comme traces (logs personnalisés) à Application Insights.</li>
                    <li><strong>Alertes :</strong> Une règle d'alerte est configurée dans Application Insights pour notifier l'équipe si le nombre de prédictions incorrectes signalées dépasse un certain seuil sur une période donnée (ex: >3 erreurs en 5 minutes).</li>
                </ul>
            </p>
            <figure>
                <img src="img/alerte_azure.png" alt="Ressource d'alerte pour les feedbacks dans Application Insights" class="screenshot">
                <figcaption>Figure 7 : Ressource d'alerte pour analyser les feedbacks dans Application Insights.</figcaption>
            </figure>
            <figure>
                <img src="img/mail.png" alt="Ressource d'alerte pour les feedbacks dans Application Insights" class="screenshot">
                <figcaption>Figure 7 : Ressource d'alerte pour analyser les feedbacks dans Application Insights.</figcaption>
            </figure>
            <p>
                Cette infrastructure de monitoring est la première étape vers un système d'amélioration continue, où les erreurs identifiées peuvent être utilisées pour affiner et ré-entraîner le modèle.
            </p>

            <div class="conclusion">
                <h2>4. Conclusion et Perspectives</h2>
                <p>
                    Ce projet a permis de développer et d'évaluer plusieurs approches pour l'analyse de sentiments de tweets, aboutissant au déploiement d'un modèle LSTM via une API robuste. L'implémentation d'une démarche MLOps, incluant le suivi avec MLflow, le versioning avec Git, l'automatisation CI/CD avec GitHub Actions, et l'initialisation du monitoring avec Azure Application Insights, constitue une base solide pour des développements futurs.
                </p>
                <p>
                    Les prochaines étapes pourraient inclure l'enrichissement du dataset avec les feedbacks collectés, l'exploration de techniques de détection de dérive du modèle (model drift), et l'automatisation plus poussée du cycle de ré-entraînement. La comparaison continue avec des modèles plus complexes comme BERT, en tenant compte des contraintes de production, reste également une piste d'amélioration.
                </p>
            </div>
        </article>
    </div>

    <footer>
        <p>Documentation technique du projet d'analyse de sentiments - LECROART Antoine - 2025></p>
    </footer>
</body>
</html>