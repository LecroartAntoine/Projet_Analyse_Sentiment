name: Build and deploy Python project to Azure Function App - Projet-Analyse-Sentiment

on:
  push:
    branches:
      - main
  workflow_dispatch:

env: # Workflow-level environment variables, accessible in all jobs
  PYTHON_VERSION: '3.10'
  AZURE_FUNCTIONAPP_ROOT_FOLDER: 'api' # Base path for function app code
  MLFLOW_REGISTERED_MODEL_NAME: 'MODEL_ADVANCED'
  MLFLOW_TRACKING_URI_RELATIVE: 'Mod√©lisation/mlruns' 
  ARTIFACT_ZIP_NAME: 'python-app.zip'

jobs:
  build:
    runs-on: ubuntu-latest
    permissions: 
      contents: read
    env: # Job-level environment variables for the 'build' job
        # We reconstruct the path or use github.workspace directly
        # AZURE_FUNCTIONAPP_ROOT_FOLDER_JOB: 'api' # If you wanted to override or specify differently at job level
        FUNCTION_APP_MODEL_DIR_ABSOLUTE: ${{ github.workspace }}/api/model # Construct full path directly
                                                                      # Using the known value 'api' from workflow env

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 

      - name: Setup Python version
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }} # Workflow env is fine here

      - name: Install MLflow and Function App dependencies
        run: |
          python -m pip install --upgrade pip
          pip install mlflow 
          pip install -r ${{ env.AZURE_FUNCTIONAPP_ROOT_FOLDER }}/requirements.txt # Workflow env is fine here
      
      - name: Create target model directory in Function App folder
        # Use the job-level 'FUNCTION_APP_MODEL_DIR_ABSOLUTE' which was constructed correctly
        run: mkdir -p ${{ env.FUNCTION_APP_MODEL_DIR_ABSOLUTE }}

      - name: Download latest registered model and tokenizer from MLflow
        run: |
          echo "Setting MLFLOW_TRACKING_URI to local path: file://${{ github.workspace }}/${{ env.MLFLOW_TRACKING_URI_RELATIVE }}"
          export MLFLOW_TRACKING_URI_FOR_SCRIPT="file://${{ github.workspace }}/${{ env.MLFLOW_TRACKING_URI_RELATIVE }}"
          
          echo "Fetching latest version for model: ${{ env.MLFLOW_REGISTERED_MODEL_NAME }}"
          
          python -c "
          import mlflow
          import shutil
          import os

          model_name = os.environ.get('MLFLOW_REGISTERED_MODEL_NAME_SCRIPT')
          target_model_dir = os.environ.get('FUNCTION_APP_MODEL_DIR_ABSOLUTE_SCRIPT') # This will receive the job-level env
          mlflow_tracking_uri = os.environ.get('MLFLOW_TRACKING_URI_FOR_SCRIPT')

          if not all([model_name, target_model_dir, mlflow_tracking_uri]):
              print('One or more critical environment variables for Python script not set!')
              print(f'model_name: {model_name}')
              print(f'target_model_dir: {target_model_dir}')
              print(f'mlflow_tracking_uri: {mlflow_tracking_uri}')
              exit(1)

          mlflow.set_tracking_uri(mlflow_tracking_uri)
          print(f'MLflow tracking URI set to: {mlflow.get_tracking_uri()}')
          
          client = mlflow.tracking.MlflowClient()
          
          try:
              latest_version_info = client.get_latest_versions(model_name, stages=['None']) 
              if not latest_version_info:
                  print(f'No versions found for model {model_name} with stage None. Trying Production stage...')
                  latest_version_info = client.get_latest_versions(model_name, stages=['Production'])
                  if not latest_version_info:
                      print(f'No versions found for model {model_name} with stage Production either.')
                      exit(1)

              latest_model = latest_version_info[0]
              run_id = latest_model.run_id
              print(f'Latest version for {model_name}: Version {latest_model.version}, Run ID: {run_id}, Source: {latest_model.source}')

              final_model_path = os.path.join(target_model_dir, 'model.keras')
              final_tokenizer_path = os.path.join(target_model_dir, 'keras_tokenizer.pkl')

              print(f'Attempting to download model artifact: model/data/model.keras from run {run_id} to {target_model_dir}')
              mlflow.artifacts.download_artifacts(
                  run_id=run_id,
                  artifact_path='model/data/model.keras', 
                  dst_path=target_model_dir  
              )
              downloaded_model_subpath = os.path.join(target_model_dir, 'model', 'data', 'model.keras')
              if os.path.exists(downloaded_model_subpath):
                  shutil.move(downloaded_model_subpath, final_model_path)
                  print(f'Moved Keras model from {downloaded_model_subpath} to {final_model_path}')
                  if os.path.isdir(os.path.join(target_model_dir, 'model', 'data')):
                      try: os.rmdir(os.path.join(target_model_dir, 'model', 'data'))
                      except OSError: pass 
                  if os.path.isdir(os.path.join(target_model_dir, 'model')):
                      try: os.rmdir(os.path.join(target_model_dir, 'model'))
                      except OSError: pass 
              elif os.path.exists(final_model_path):
                  print(f'Keras model already at {final_model_path}')
              else:
                  print(f'Keras model file not found at {downloaded_model_subpath} or {final_model_path}. Contents of {target_model_dir}:')
                  os.system(f'ls -lR {target_model_dir}')
                  exit(1)

              print(f'Attempting to download tokenizer artifact: tokenizer/keras_tokenizer.pkl from run {run_id} to {target_model_dir}')
              mlflow.artifacts.download_artifacts(
                  run_id=run_id,
                  artifact_path='tokenizer/keras_tokenizer.pkl', 
                  dst_path=target_model_dir 
              )
              downloaded_tokenizer_subpath = os.path.join(target_model_dir, 'tokenizer', 'keras_tokenizer.pkl')
              if os.path.exists(downloaded_tokenizer_subpath):
                  shutil.move(downloaded_tokenizer_subpath, final_tokenizer_path)
                  print(f'Moved tokenizer from {downloaded_tokenizer_subpath} to {final_tokenizer_path}')
                  if os.path.isdir(os.path.join(target_model_dir, 'tokenizer')):
                      try: os.rmdir(os.path.join(target_model_dir, 'tokenizer'))
                      except OSError: pass 
              elif os.path.exists(final_tokenizer_path):
                  print(f'Tokenizer already at {final_tokenizer_path}')
              else:
                  print(f'Tokenizer file not found at {downloaded_tokenizer_subpath} or {final_tokenizer_path}. Contents of {target_model_dir}:')
                  os.system(f'ls -lR {target_model_dir}')
                  exit(1)

              print(f'Model and tokenizer should be in {target_model_dir}')
              print('Final contents of target model directory:')
              os.system(f'ls -lR {target_model_dir}')

          except Exception as e:
              print(f'Error fetching/downloading model: {e}')
              import traceback
              traceback.print_exc()
              exit(1)
          "
        env: # Env for this specific run step (python script)
          MLFLOW_REGISTERED_MODEL_NAME_SCRIPT: ${{ env.MLFLOW_REGISTERED_MODEL_NAME }} # Uses workflow env
          FUNCTION_APP_MODEL_DIR_ABSOLUTE_SCRIPT: ${{ env.FUNCTION_APP_MODEL_DIR_ABSOLUTE }} # Uses job env (which was correctly constructed)
          # MLFLOW_TRACKING_URI_FOR_SCRIPT is exported in the shell script part of this run step

      - name: Zip artifact for deployment (Function App folder contents)
        run: |
          echo "Current directory before zipping: $(pwd)"
          echo "Zipping contents of ${{ env.AZURE_FUNCTIONAPP_ROOT_FOLDER }} into ../${{ env.ARTIFACT_ZIP_NAME }}"
          cd ${{ env.AZURE_FUNCTIONAPP_ROOT_FOLDER }} # Uses workflow env
          zip -r ../${{ env.ARTIFACT_ZIP_NAME }} . -x "*venv/*" -x "*.git/*" -x "*__pycache__/*" # Uses workflow env
          cd .. 
          echo "Zip file created at: $(pwd)/${{ env.ARTIFACT_ZIP_NAME }}" # Uses workflow env
          ls -l ${{ env.ARTIFACT_ZIP_NAME }} # Uses workflow env


      - name: Upload artifact for deployment job
        uses: actions/upload-artifact@v4
        with:
          name: python-app-artifact
          path: ${{ env.ARTIFACT_ZIP_NAME }} # Uses workflow env

  deploy:
    runs-on: ubuntu-latest
    needs: build
    environment:
      name: 'Production'
      url: ${{ steps.deploy-to-function.outputs.webapp-url }}
    
    steps:
      - name: Download artifact from build job
        uses: actions/download-artifact@v4
        with:
          name: python-app-artifact

      - name: List files after download
        run: ls -lR

      - name: 'Deploy to Azure Functions'
        uses: Azure/functions-action@v1
        id: deploy-to-function
        with:
          app-name: 'Projet-Analyse-Sentiment'
          slot-name: 'Production'
          package: ${{ env.ARTIFACT_ZIP_NAME }} # Uses workflow env
          publish-profile: ${{ secrets.AZUREAPPSERVICE_PUBLISHPROFILE_18963F46B19E49D5A57355029EB94F04 }}