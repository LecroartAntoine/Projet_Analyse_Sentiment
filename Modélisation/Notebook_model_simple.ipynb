{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Projet 7 : Réalisez une analyse de sentiments grâce au Deep Learning](#toc0_)\n",
    "# <a id='toc2_'></a>[Modèle sur mesure simple](#toc0_)\n",
    "\n",
    "[Lien OpenClassroom](https://openclassrooms.com/fr/paths/795/projects/1516/1578-mission)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0a5b7",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Projet 7 : Réalisez une analyse de sentiments grâce au Deep Learning](#toc1_)    \n",
    "- [Modèle sur mesure simple](#toc2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbb27ab",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## <a id='toc2_1_'></a>[Imports](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df1d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn  # Needed for autologging or specific sklearn logging\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import joblib  # For saving the vectorizer\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c25120",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## <a id='toc2_2_'></a>[Chargement des données](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f48e53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully:\n",
      "Train samples: 1113546\n",
      "Validation samples: 238617\n",
      "Test samples: 238618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeBonCassoulet\\AppData\\Local\\Temp\\ipykernel_14628\\2157717661.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df[\"cleaned_text\"].fillna(\"\", inplace=True)\n",
      "C:\\Users\\LeBonCassoulet\\AppData\\Local\\Temp\\ipykernel_14628\\2157717661.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  val_df[\"cleaned_text\"].fillna(\"\", inplace=True)\n",
      "C:\\Users\\LeBonCassoulet\\AppData\\Local\\Temp\\ipykernel_14628\\2157717661.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df[\"cleaned_text\"].fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_PATH = \"./train_data.csv\"\n",
    "VAL_DATA_PATH = \"./validation_data.csv\"\n",
    "TEST_DATA_PATH = \"./test_data.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "val_df = pd.read_csv(VAL_DATA_PATH)\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "# Handle potential NaN values in 'cleaned_text' that might result from preprocessing\n",
    "train_df[\"cleaned_text\"].fillna(\"\", inplace=True)\n",
    "val_df[\"cleaned_text\"].fillna(\"\", inplace=True)\n",
    "test_df[\"cleaned_text\"].fillna(\"\", inplace=True)\n",
    "\n",
    "\n",
    "X_train = train_df[\"cleaned_text\"]\n",
    "y_train = train_df[\"sentiment\"]\n",
    "X_val = val_df[\"cleaned_text\"]\n",
    "y_val = val_df[\"sentiment\"]\n",
    "X_test = test_df[\"cleaned_text\"]\n",
    "y_test = test_df[\"sentiment\"]\n",
    "\n",
    "print(\"Data loaded successfully:\")\n",
    "print(f\"Train samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913b3458",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Préparation de MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6945cef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing experiment 'Tweet Sentiment Analysis - Simple Models' with ID: 897408299388468996\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = \"Tweet Sentiment Analysis - Simple Models\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Get the current experiment details (optional)\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        experiment_id = experiment.experiment_id\n",
    "        print(f\"Using existing experiment '{EXPERIMENT_NAME}' with ID: {experiment_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up MLflow experiment: {e}\")\n",
    "    raise\n",
    "\n",
    "VECTORIZER_FILENAME = \"tfidf_vectorizer_simple.joblib\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdfeeea",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Extraction des features, entrainement du modèle et logging MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d226bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting MLflow Run: LogisticRegression_TFIDF\n",
      "MLflow Run ID: e07074f03a6d488cb53c11f435aa0f68\n",
      "Logging parameters...\n",
      "Fitting TF-IDF Vectorizer...\n",
      "TF-IDF - Training data transformed shape: (1113546, 10000)\n",
      "TF-IDF - Validation data transformed shape: (238617, 10000)\n",
      "TF-IDF - Test data transformed shape: (238618, 10000)\n",
      "Vectorizer saved locally to tfidf_vectorizer_simple.joblib\n",
      "Vectorizer logged as MLflow artifact.\n",
      "Training Logistic Regression model...\n",
      "Model training complete.\n",
      "Evaluating on validation set...\n",
      "Logging validation metrics...\n",
      "Validation Accuracy: 0.7750\n",
      "Validation Precision: 0.7633\n",
      "Validation Recall: 0.7971\n",
      "Validation F1-Score: 0.7798\n",
      "Logging the trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/25 16:22:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged successfully.\n",
      "\n",
      "MLflow Run e07074f03a6d488cb53c11f435aa0f68 finished.\n"
     ]
    }
   ],
   "source": [
    "run_name = \"LogisticRegression_TFIDF\"\n",
    "print(f\"\\nStarting MLflow Run: {run_name}\")\n",
    "\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"MLflow Run ID: {run_id}\")\n",
    "\n",
    "    # --- Parameters ---\n",
    "    # TF-IDF Parameters\n",
    "    tfidf_max_features = 10000\n",
    "    tfidf_ngram_range = (1, 3)  # Use unigrams and bigrams\n",
    "\n",
    "    # Logistic Regression Parameters\n",
    "    lr_C = 0.5  # Inverse of regularization strength\n",
    "    lr_solver = \"saga\"  # Good for smaller datasets and binary classification\n",
    "    lr_max_iter = 500\n",
    "    lr_class_weight = \"balanced\"  # Useful for imbalanced datasets\n",
    "\n",
    "    # Log parameters\n",
    "    print(\"Logging parameters...\")\n",
    "    mlflow.log_param(\"vectorizer_type\", \"TF-IDF\")\n",
    "    mlflow.log_param(\"tfidf_max_features\", tfidf_max_features)\n",
    "    mlflow.log_param(\"tfidf_ngram_range\", str(tfidf_ngram_range))  # Log tuple as string\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"lr_C\", lr_C)\n",
    "    mlflow.log_param(\"lr_solver\", lr_solver)\n",
    "    mlflow.log_param(\"lr_max_iter\", lr_max_iter)\n",
    "    mlflow.log_param(\"lr_class_weight\", lr_class_weight)\n",
    "\n",
    "    # --- Feature Extraction (TF-IDF) ---\n",
    "    print(\"Fitting TF-IDF Vectorizer...\")\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=tfidf_max_features, ngram_range=tfidf_ngram_range\n",
    "    )\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    print(f\"TF-IDF - Training data transformed shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "    # Transform validation and test sets\n",
    "    X_val_tfidf = vectorizer.transform(X_val)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    print(f\"TF-IDF - Validation data transformed shape: {X_val_tfidf.shape}\")\n",
    "    print(f\"TF-IDF - Test data transformed shape: {X_test_tfidf.shape}\")\n",
    "\n",
    "    # Save the fitted vectorizer locally first\n",
    "    joblib.dump(vectorizer, VECTORIZER_FILENAME)\n",
    "    print(f\"Vectorizer saved locally to {VECTORIZER_FILENAME}\")\n",
    "\n",
    "    # Log the vectorizer as an artifact\n",
    "    mlflow.log_artifact(VECTORIZER_FILENAME, artifact_path=\"vectorizer\")\n",
    "    print(\"Vectorizer logged as MLflow artifact.\")\n",
    "\n",
    "    # Clean up local file after logging (optional)\n",
    "    if os.path.exists(VECTORIZER_FILENAME):\n",
    "        os.remove(VECTORIZER_FILENAME)\n",
    "\n",
    "    # --- Model Training ---\n",
    "    print(\"Training Logistic Regression model...\")\n",
    "    model = LogisticRegression(\n",
    "        C=lr_C,\n",
    "        solver=lr_solver,\n",
    "        max_iter=lr_max_iter,\n",
    "        class_weight=lr_class_weight,\n",
    "        random_state=42,  # for reproducibility\n",
    "    )\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # --- Evaluation on Validation Set ---\n",
    "    print(\"Evaluating on validation set...\")\n",
    "    y_val_pred = model.predict(X_val_tfidf)\n",
    "    y_val_pred_proba = model.predict_proba(X_val_tfidf)[\n",
    "        :, 1\n",
    "    ]  # Probability of positive class\n",
    "\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_precision = precision_score(y_val, y_val_pred, zero_division=0)\n",
    "    val_recall = recall_score(y_val, y_val_pred, zero_division=0)\n",
    "    val_f1 = f1_score(y_val, y_val_pred, zero_division=0)\n",
    "\n",
    "    # Log validation metrics\n",
    "    print(\"Logging validation metrics...\")\n",
    "    mlflow.log_metric(\"val_accuracy\", val_accuracy)\n",
    "    mlflow.log_metric(\"val_precision\", val_precision)\n",
    "    mlflow.log_metric(\"val_recall\", val_recall)\n",
    "    mlflow.log_metric(\"val_f1\", val_f1)\n",
    "\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "    print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "    print(f\"Validation F1-Score: {val_f1:.4f}\")\n",
    "\n",
    "    # --- Log the Model ---\n",
    "    print(\"Logging the trained model...\")\n",
    "    mlflow.sklearn.log_model(model, artifact_path=\"logistic-regression-model\")\n",
    "    print(\"Model logged successfully.\")\n",
    "\n",
    "    val_report = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "    mlflow.log_dict(val_report, \"validation_classification_report.json\")\n",
    "\n",
    "\n",
    "print(f\"\\nMLflow Run {run_id} finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b4194",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Évaluation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc5eb1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Évaluation sur les données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d46a1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from: runs:/e07074f03a6d488cb53c11f435aa0f68/logistic-regression-model\n",
      "\n",
      "Test Set Performance:\n",
      "Test Accuracy: 0.7744\n",
      "Test Precision: 0.7625\n",
      "Test Recall: 0.7969\n",
      "Test F1-Score: 0.7793\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.75      0.77    119351\n",
      "           1       0.76      0.80      0.78    119267\n",
      "\n",
      "    accuracy                           0.77    238618\n",
      "   macro avg       0.77      0.77      0.77    238618\n",
      "weighted avg       0.77      0.77      0.77    238618\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[89747 29604]\n",
      " [24222 95045]]\n",
      "\n",
      "Test metrics logged back to the MLflow run.\n"
     ]
    }
   ],
   "source": [
    "logged_model_uri = f\"runs:/{run_id}/logistic-regression-model\"\n",
    "\n",
    "\n",
    "# Load the model logged in the previous run\n",
    "try:\n",
    "    loaded_model = mlflow.sklearn.load_model(logged_model_uri)\n",
    "    print(f\"Model loaded successfully from: {logged_model_uri}\")\n",
    "\n",
    "    # Make predictions on the test set (using the same TF-IDF transformation)\n",
    "    y_test_pred = loaded_model.predict(X_test_tfidf)\n",
    "    y_test_pred_proba = loaded_model.predict_proba(X_test_tfidf)[\n",
    "        :, 1\n",
    "    ]  # Probability of positive class\n",
    "\n",
    "    # Calculate test metrics\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report (Test Set):\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "    print(\"\\nConfusion Matrix (Test Set):\")\n",
    "    print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "    # --- Optional: Log test metrics back to the same MLflow run ---\n",
    "    # This is often done to have all metrics associated with a single run.\n",
    "    # You need the client to log to an existing run *after* it has finished.\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    client.log_metric(run_id, \"test_accuracy\", test_accuracy)\n",
    "    client.log_metric(run_id, \"test_precision\", test_precision)\n",
    "    client.log_metric(run_id, \"test_recall\", test_recall)\n",
    "    client.log_metric(run_id, \"test_f1\", test_f1)\n",
    "    print(\"\\nTest metrics logged back to the MLflow run.\")\n",
    "\n",
    "    # Log test classification report as well\n",
    "    test_report_dict = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "    client.log_dict(run_id, test_report_dict, \"test_classification_report.json\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or evaluating on test set: {e}\")\n",
    "    print(\"Ensure the run ID is correct and the model was logged properly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ddd88",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Comparaison avec un modèle naif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "976ea7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Baseline Performance (Test Set):\n",
      "Baseline Accuracy: 0.5002\n",
      "Baseline Precision: 0.0000\n",
      "Baseline Recall: 0.0000\n",
      "Baseline F1-Score: 0.0000\n",
      "\n",
      "Comparison:\n",
      "Model Test Accuracy: 0.7744 vs Baseline: 0.5002\n",
      "Model Test F1-Score: 0.7793 vs Baseline: 0.0000\n"
     ]
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train_tfidf, y_train)  # Needs to be fitted, though strategy is simple\n",
    "\n",
    "y_test_pred_dummy = dummy_clf.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate metrics for the baseline\n",
    "dummy_accuracy = accuracy_score(y_test, y_test_pred_dummy)\n",
    "dummy_precision = precision_score(y_test, y_test_pred_dummy, zero_division=0)\n",
    "dummy_recall = recall_score(y_test, y_test_pred_dummy, zero_division=0)\n",
    "dummy_f1 = f1_score(y_test, y_test_pred_dummy, zero_division=0)\n",
    "\n",
    "print(\"Naive Baseline Performance (Test Set):\")\n",
    "print(f\"Baseline Accuracy: {dummy_accuracy:.4f}\")\n",
    "print(f\"Baseline Precision: {dummy_precision:.4f}\")\n",
    "print(f\"Baseline Recall: {dummy_recall:.4f}\")\n",
    "print(f\"Baseline F1-Score: {dummy_f1:.4f}\")\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Model Test Accuracy: {test_accuracy:.4f} vs Baseline: {dummy_accuracy:.4f}\")\n",
    "print(f\"Model Test F1-Score: {test_f1:.4f} vs Baseline: {dummy_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ed6e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Enregistrement du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5010784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model registered successfully:\n",
      "- Name: MODEL_SIMPLE\n",
      "- Version: 2\n",
      "- Stage: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'MODEL_SIMPLE' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'MODEL_SIMPLE'.\n"
     ]
    }
   ],
   "source": [
    "registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_model_uri, name=\"MODEL_SIMPLE\"\n",
    ")\n",
    "print(\"Model registered successfully:\")\n",
    "print(f\"- Name: {registered_model_info.name}\")\n",
    "print(f\"- Version: {registered_model_info.version}\")\n",
    "print(f\"- Stage: {registered_model_info.current_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a387d2f",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Dashboard MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "427ce48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! mlflow server --host 127.0.0.1 --port 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9cfca8",
   "metadata": {},
   "source": [
    "![Overview](./mlflow_screenshot/simple/Overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9482467",
   "metadata": {},
   "source": [
    "![Metrics](./mlflow_screenshot/simple/Metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36019d12",
   "metadata": {},
   "source": [
    "![Compare Runs](./mlflow_screenshot/simple/Compare_runs.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
