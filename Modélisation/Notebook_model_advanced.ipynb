{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Projet 7 : Réalisez une analyse de sentiments grâce au Deep Learning](#toc0_)\n",
    "# <a id='toc2_'></a>[Modèle sur mesure avancé](#toc0_)\n",
    "\n",
    "[Lien OpenClassroom](https://openclassrooms.com/fr/paths/795/projects/1516/1578-mission)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0a5b7",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Projet 7 : Réalisez une analyse de sentiments grâce au Deep Learning](#toc1_)    \n",
    "- [Modèle sur mesure simple](#toc2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbb27ab",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## <a id='toc2_1_'></a>[Imports](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1df1d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Bidirectional,\n",
    "    Input,\n",
    "    SpatialDropout1D,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import mlflow\n",
    "import mlflow.tensorflow  # Essential for autologging\n",
    "import pickle  # For saving the tokenizer\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.get_logger().setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c25120",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## <a id='toc2_2_'></a>[Chargement des données](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f48e53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully:\n",
      "Train samples: 1113546\n",
      "Validation samples: 238617\n",
      "Test samples: 238618\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_PATH = \"./train_data.csv\"\n",
    "VAL_DATA_PATH = \"./validation_data.csv\"\n",
    "TEST_DATA_PATH = \"./test_data.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "val_df = pd.read_csv(VAL_DATA_PATH)\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "# Handle potential NaN values in 'cleaned_text' that might result from preprocessing\n",
    "train_df[\"cleaned_text\"].fillna(\"\", inplace=True)\n",
    "val_df[\"cleaned_text\"].fillna(\"\", inplace=True)\n",
    "test_df[\"cleaned_text\"].fillna(\"\", inplace=True)\n",
    "\n",
    "\n",
    "X_train = train_df[\"cleaned_text\"]\n",
    "y_train = train_df[\"sentiment\"]\n",
    "X_val = val_df[\"cleaned_text\"]\n",
    "y_val = val_df[\"sentiment\"]\n",
    "X_test = test_df[\"cleaned_text\"]\n",
    "y_test = test_df[\"sentiment\"]\n",
    "\n",
    "print(\"Data loaded successfully:\")\n",
    "print(f\"Train samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34c7d5e",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Préparation pour Deep Leanring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c617c5c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Création d'un Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754d1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual vocabulary size used: 10000\n",
      "Shape of padded training sequences: (1113546, 100)\n",
      "Shape of padded validation sequences: (238617, 100)\n",
      "Shape of padded test sequences: (238618, 100)\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=VOCAB_SIZE, oov_token=\"<OOV>\"\n",
    ")  # OOV token for out-of-vocabulary words\n",
    "\n",
    "# Fit the tokenizer ONLY on the training data\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text data to sequences of integers\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X_train_pad = pad_sequences(\n",
    "    X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\", truncating=\"post\"\n",
    ")\n",
    "X_val_pad = pad_sequences(\n",
    "    X_val_seq, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\", truncating=\"post\"\n",
    ")\n",
    "X_test_pad = pad_sequences(\n",
    "    X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\", truncating=\"post\"\n",
    ")\n",
    "\n",
    "# Vocabulary size for the embedding layer (add 1 for the padding token 0)\n",
    "# Use min to handle cases where actual vocab is smaller than VOCAB_SIZE\n",
    "actual_vocab_size = min(VOCAB_SIZE, len(tokenizer.word_index) + 1)\n",
    "print(f\"Actual vocabulary size used: {actual_vocab_size}\")\n",
    "print(f\"Shape of padded training sequences: {X_train_pad.shape}\")\n",
    "print(f\"Shape of padded validation sequences: {X_val_pad.shape}\")\n",
    "print(f\"Shape of padded test sequences: {X_test_pad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb24600",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sauvegarde du Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8433580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved locally to keras_tokenizer.pkl \n"
     ]
    }
   ],
   "source": [
    "with open(\"keras_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04334440",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### MLFlow Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f742b283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/25 16:14:51 INFO mlflow.tracking.fluent: Experiment with name 'Tweet Sentiment Analysis - Advanced DL' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow experiment set to: 'Tweet Sentiment Analysis - Advanced DL'\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = \"Tweet Sentiment Analysis - Advanced DL\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "print(f\"MLflow experiment set to: '{EXPERIMENT_NAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89208305",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Experiment 1: LSTM avec GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d2432",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Chargement de GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142412d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors in ./glove.6B.300d.txt.\n",
      "Creating embedding matrix...\n",
      "Converted 9226 words (773 misses)\n",
      "Shape of embedding matrix: (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "GLOVE_PATH = \"./glove.6B.300d.txt\"\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "embeddings_index = {}\n",
    "try:\n",
    "    with open(GLOVE_PATH, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    print(f\"Found {len(embeddings_index)} word vectors in {GLOVE_PATH}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: GloVe file not found at {GLOVE_PATH}\")\n",
    "    print(\"Skipping GloVe experiment.\")\n",
    "    embeddings_index = None  # Ensure variable exists but is None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading GloVe file: {e}\")\n",
    "    embeddings_index = None\n",
    "\n",
    "embedding_matrix = None\n",
    "if embeddings_index:\n",
    "    print(\"Creating embedding matrix...\")\n",
    "    # Initialize matrix with zeros\n",
    "    embedding_matrix = np.zeros((actual_vocab_size, EMBEDDING_DIM))\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    # Populate the matrix with GloVe vectors for words in our tokenizer's vocabulary\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i >= actual_vocab_size:  # Skip words beyond our vocab size limit\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print(f\"Converted {hits} words ({misses} misses)\")\n",
    "    print(f\"Shape of embedding matrix: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd908a97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    max_length,\n",
    "    lstm_units,\n",
    "    dropout_rate,\n",
    "    spatial_dropout_rate,\n",
    "    learning_rate,\n",
    "    embedding_matrix=None,\n",
    "    is_embedding_trainable=False,\n",
    "):\n",
    "    \"\"\"Builds a Keras LSTM model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(max_length,)))  # Explicit Input layer\n",
    "\n",
    "    # Embedding Layer\n",
    "    if embedding_matrix is not None:\n",
    "        print(\"Using pre-trained embedding matrix.\")\n",
    "        model.add(\n",
    "            Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=embedding_dim,\n",
    "                weights=[embedding_matrix],\n",
    "                input_length=max_length,\n",
    "                trainable=is_embedding_trainable,  # Typically False for pre-trained\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\"Using trainable embedding layer.\")\n",
    "        model.add(\n",
    "            Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=embedding_dim,\n",
    "                input_length=max_length,\n",
    "                trainable=is_embedding_trainable,  # Typically True if learning from scratch\n",
    "            )\n",
    "        )\n",
    "\n",
    "    model.add(\n",
    "        SpatialDropout1D(spatial_dropout_rate)\n",
    "    )  # Helps prevent overfitting in NLP\n",
    "\n",
    "    # Using Bidirectional LSTM for potentially better context capture\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate)\n",
    "        )\n",
    "    )\n",
    "    # Or standard LSTM: model.add(LSTM(lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate))\n",
    "\n",
    "    # Optional Dense layer before output\n",
    "    # model.add(Dense(32, activation='relu'))\n",
    "    # model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))  # Output layer for binary classification\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "    )  # Add more metrics like Precision, Recall if needed during training\n",
    "\n",
    "    print(\"\\nModel Summary:\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a070458",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Entrainement du modèle avec MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ea880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting MLflow Run for: LSTM_GloVe_Embeddings ---\n",
      "MLflow Run ID (GloVe): dcabf68fdc9b49f38831717b72983354\n",
      "Using pre-trained embedding matrix.\n",
      "\n",
      "Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">186,880</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │     \u001b[38;5;34m3,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m186,880\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,187,009</span> (12.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,187,009\u001b[0m (12.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">187,009</span> (730.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m187,009\u001b[0m (730.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> (11.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,000,000\u001b[0m (11.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/25 16:16:11 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM model with GloVe embeddings...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 57\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# MLflow callback is handled by autolog()\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# --- Train the model ---\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining LSTM model with GloVe embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m history_glove \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_glove\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Autolog handles MLflow logging callback\u001b[39;49;00m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Set to 1 or 2 for progress updates\u001b[39;49;00m\n\u001b[0;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGloVe Model Training Finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# --- Manually log the tokenizer artifact ---\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Autolog doesn't handle custom artifacts like the tokenizer pickle file\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LeBonCassoulet\\Documents\\GitHub\\OpenClassrooms\\Projet_7\\.venv\\lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:483\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m call_original \u001b[38;5;241m=\u001b[39m update_wrapper_extended(call_original, original)\n\u001b[0;32m    481\u001b[0m event_logger\u001b[38;5;241m.\u001b[39mlog_patch_function_start(args, kwargs)\n\u001b[1;32m--> 483\u001b[0m patch_function(call_original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    485\u001b[0m session\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msucceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m event_logger\u001b[38;5;241m.\u001b[39mlog_patch_function_success(args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\LeBonCassoulet\\Documents\\GitHub\\OpenClassrooms\\Projet_7\\.venv\\lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:182\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001b[1;34m(original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     managed_run \u001b[38;5;241m=\u001b[39m create_managed_run()\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     result \u001b[38;5;241m=\u001b[39m patch_function(original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# that runs are terminated if a user prematurely interrupts training execution\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# (e.g. via sigint / ctrl-c)\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m managed_run:\n",
      "File \u001b[1;32mc:\\Users\\LeBonCassoulet\\Documents\\GitHub\\OpenClassrooms\\Projet_7\\.venv\\lib\\site-packages\\mlflow\\tensorflow\\__init__.py:1378\u001b[0m, in \u001b[0;36mautolog.<locals>._patched_inference\u001b[1;34m(original, inst, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1374\u001b[0m         shutil\u001b[38;5;241m.\u001b[39mrmtree(log_dir\u001b[38;5;241m.\u001b[39mlocation)\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;66;03m# Regardless of what happens during the `_on_exception` callback, reraise\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;66;03m# the original implementation exception once the callback completes\u001b[39;00m\n\u001b[1;32m-> 1378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\LeBonCassoulet\\Documents\\GitHub\\OpenClassrooms\\Projet_7\\.venv\\lib\\site-packages\\mlflow\\tensorflow\\__init__.py:1349\u001b[0m, in \u001b[0;36mautolog.<locals>._patched_inference\u001b[1;34m(original, inst, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1344\u001b[0m         _logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   1345\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to log training dataset information to MLflow Tracking. Reason: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1346\u001b[0m             e,\n\u001b[0;32m   1347\u001b[0m         )\n\u001b[1;32m-> 1349\u001b[0m history \u001b[38;5;241m=\u001b[39m original(inst, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_models:\n\u001b[0;32m   1352\u001b[0m     _log_keras_model(history, args)\n",
      "File \u001b[1;32mc:\\Users\\LeBonCassoulet\\Documents\\GitHub\\OpenClassrooms\\Projet_7\\.venv\\lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:474\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[1;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[0;32m    471\u001b[0m         original_result \u001b[38;5;241m=\u001b[39m original(\u001b[38;5;241m*\u001b[39m_og_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_og_kwargs)\n\u001b[0;32m    472\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n\u001b[1;32m--> 474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_original_fn_with_event_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_original_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LeBonCassoulet\\Documents\\GitHub\\OpenClassrooms\\Projet_7\\.venv\\lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:425\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[1;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    423\u001b[0m     event_logger\u001b[38;5;241m.\u001b[39mlog_original_function_start(og_args, og_kwargs)\n\u001b[1;32m--> 425\u001b[0m     original_fn_result \u001b[38;5;241m=\u001b[39m original_fn(\u001b[38;5;241m*\u001b[39mog_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mog_kwargs)\n\u001b[0;32m    427\u001b[0m     event_logger\u001b[38;5;241m.\u001b[39mlog_original_function_success(og_args, og_kwargs)\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[1;32mc:\\Users\\LeBonCassoulet\\Documents\\GitHub\\OpenClassrooms\\Projet_7\\.venv\\lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:471\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[1;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;66;03m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m NonMlflowWarningsBehaviorForCurrentThread(\n\u001b[0;32m    468\u001b[0m     disable_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    469\u001b[0m     reroute_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    470\u001b[0m ):\n\u001b[1;32m--> 471\u001b[0m     original_result \u001b[38;5;241m=\u001b[39m original(\u001b[38;5;241m*\u001b[39m_og_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_og_kwargs)\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n",
      "File \u001b[1;32mc:\\Users\\LeBonCassoulet\\Documents\\GitHub\\OpenClassrooms\\Projet_7\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\LeBonCassoulet\\Documents\\GitHub\\OpenClassrooms\\Projet_7\\.venv\\lib\\site-packages\\optree\\ops.py:766\u001b[0m, in \u001b[0;36mtree_map\u001b[1;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[0;32m    764\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[0;32m    765\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[1;32m--> 766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid dtype: object"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "LSTM_UNITS = 64\n",
    "DROPOUT_RATE = 0.3\n",
    "SPATIAL_DROPOUT_RATE = 0.3\n",
    "\n",
    "# Training Parameters\n",
    "EPOCHS = 10  # Max number of epochs\n",
    "BATCH_SIZE = 64  # Batch size for training\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "TOKENIZER_ARTIFACT_PATH = \"tokenizer\"\n",
    "MODEL_ARTIFACT_PATH = \"model\"\n",
    "\n",
    "run_name_glove = \"LSTM_GloVe_Embeddings\"\n",
    "print(f\"\\n--- Starting MLflow Run for: {run_name_glove} ---\")\n",
    "\n",
    "# Enable MLflow autologging for TensorFlow/Keras\n",
    "# This automatically logs parameters, metrics per epoch, the model, etc.\n",
    "mlflow.tensorflow.autolog(\n",
    "    log_models=True, disable=False, registered_model_name=None\n",
    ")  # Disable registration via autolog for now\n",
    "\n",
    "with mlflow.start_run(run_name=run_name_glove) as run_glove:\n",
    "    run_id_glove = run_glove.info.run_id\n",
    "    print(f\"MLflow Run ID (GloVe): {run_id_glove}\")\n",
    "\n",
    "    # --- Log additional parameters manually (autolog might miss some) ---\n",
    "    mlflow.log_param(\"embedding_type\", \"GloVe (Not Trainable)\")\n",
    "    mlflow.log_param(\"vocab_size\", actual_vocab_size)\n",
    "    mlflow.log_param(\"max_sequence_length\", MAX_SEQUENCE_LENGTH)\n",
    "    mlflow.log_param(\"embedding_dim\", EMBEDDING_DIM)\n",
    "    mlflow.log_param(\"lstm_units\", LSTM_UNITS)\n",
    "    mlflow.log_param(\"dropout_rate\", DROPOUT_RATE)\n",
    "    mlflow.log_param(\"spatial_dropout_rate\", SPATIAL_DROPOUT_RATE)\n",
    "    mlflow.log_param(\"learning_rate\", LEARNING_RATE)\n",
    "    mlflow.log_param(\"epochs\", EPOCHS)\n",
    "    mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "    mlflow.log_param(\"architecture\", \"Input-Embedding-SpatialDropout-BiLSTM-Dense\")\n",
    "\n",
    "    # --- Build the model ---\n",
    "    model_glove = build_lstm_model(\n",
    "        vocab_size=actual_vocab_size,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        lstm_units=LSTM_UNITS,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        spatial_dropout_rate=SPATIAL_DROPOUT_RATE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        embedding_matrix=embedding_matrix,\n",
    "        is_embedding_trainable=False,  # Crucial for using pre-trained static embeddings\n",
    "    )\n",
    "\n",
    "    # --- Callbacks ---\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
    "    )\n",
    "    # MLflow callback is handled by autolog()\n",
    "\n",
    "    # --- Train the model ---\n",
    "    print(\"\\nTraining LSTM model with GloVe embeddings...\")\n",
    "    history_glove = model_glove.fit(\n",
    "        X_train_pad,\n",
    "        y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val_pad, y_val),\n",
    "        callbacks=[early_stopping],  # Autolog handles MLflow logging callback\n",
    "        verbose=1,  # Set to 1 or 2 for progress updates\n",
    "    )\n",
    "    print(\"GloVe Model Training Finished.\")\n",
    "\n",
    "    # --- Manually log the tokenizer artifact ---\n",
    "    # Autolog doesn't handle custom artifacts like the tokenizer pickle file\n",
    "    if os.path.exists(\"keras_tokenizer.pkl\"):\n",
    "        mlflow.log_artifact(\n",
    "            \"keras_tokenizer.pkl\", artifact_path=TOKENIZER_ARTIFACT_PATH\n",
    "        )\n",
    "        print(f\"Tokenizer logged as artifact to MLflow run {run_id_glove}.\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Warning: Tokenizer file keras_tokenizer.pkl not found, could not log artifact.\"\n",
    "        )\n",
    "\n",
    "    # Autologging should have logged the model automatically at the end of training\n",
    "    print(f\"--- MLflow Run {run_id_glove} finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
